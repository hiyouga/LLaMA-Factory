# Full Fine-Tuning config for Qwen3-8B with 256k context and DeepSpeed Ulysses sequence parallelism
# Based on LIMO paper configuration: https://arxiv.org/abs/2502.03387  
# Enhanced with YARN RoPE scaling for extended context length (262144 tokens)
# Uses DeepSpeed Ulysses strategy for ultra-long context training
#
# Requirements:
#   - Log into WandB (`wandb login`) or set `report_to: none`
#   - Log into HF: `huggingface-cli login`
#   - Multi-GPU setup required for sequence parallelism
#
# Usage:
#   llamafactory-cli train qwen3_full_sft_256k.yaml

### model
model_name_or_path: Qwen/Qwen3-8B
trust_remote_code: true
rope_scaling: yarn  # YARN RoPE scaling - factor will be computed automatically
flash_attn: auto
enable_liger_kernel: true
sequence_parallel_size: 4  # Use 4 GPUs for sequence parallelism
sequence_parallel_mode: ulysses  # DeepSpeed Ulysses sequence parallel mode

### method
stage: sft
do_train: true
finetuning_type: full  # Full fine-tuning (not LoRA)
deepspeed: examples/deepspeed/ds_z3_partial_offload_config.json

### dataset
dataset: tbench_traces_sharegptv1
template: qwen  # Qwen3 template (same as Qwen2.5)
cutoff_len: 262144  # 256k context length
max_samples: null  # Use full dataset
overwrite_cache: true
preprocessing_num_workers: 64  # Match LIMO preprocessing workers
dataloader_num_workers: 8

### output  
output_dir: output/qwen3_8b_tbench_traces_256k_ulysses_ds
logging_steps: 50
save_steps: 50
plot_loss: true
overwrite_output_dir: true
save_only_model: true  # Save only model for DeepSpeed compatibility
report_to: wandb  # Set to 'none' to disable WandB

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 4  # Increased due to longer sequences
learning_rate: 1.5e-5  # Slightly lower LR for stability with long context
num_train_epochs: 3.0  # Reduced epochs due to computational cost
lr_scheduler_type: cosine
warmup_ratio: 0.1  # Longer warmup for stability
weight_decay: 0.03
max_grad_norm: 1.0e-3
bf16: true
gradient_checkpointing: true
ddp_timeout: 300000000  # Longer timeout for sequence parallelism