# Full Fine-Tuning config for Qwen3-8B with 200k context and DeepSpeed Ulysses sequence parallelism
# Based on LIMO paper configuration: https://arxiv.org/abs/2502.03387
# Enhanced with YARN RoPE scaling for extended context length (204800 tokens = 5x base model)
# Uses DeepSpeed Ulysses strategy for ultra-long context training
#
# Requirements:
#   - Log into WandB (`wandb login`) or set `report_to: none`
#   - Log into HF: `huggingface-cli login`
#   - Multi-GPU setup required for sequence parallelism
#
# Usage:
#   llamafactory-cli train qwen3_full_sft_256k.yaml

### model
model_name_or_path: Qwen/Qwen3-8B
trust_remote_code: true
rope_scaling: linear
attn: sdpa
enable_liger_kernel: true
sequence_parallel_size: 4  # Use 4 GPUs for sequence parallelism
sequence_parallel_mode: deepspeed-alst  # DeepSpeed Arctic Long Sequence Training (ALST)
alst_sequence_backend: deepspeed  # Use native DeepSpeed ALST backend
alst_ulysses_degree: 4  # Ulysses parallelism degree (should match SP size)
alst_sequence_tiling: true  # Enable sequence tiling for memory efficiency
alst_memory_optimizations: true  # Enable ALST memory optimizations

### method
stage: sft
do_train: true
finetuning_type: full  # Full fine-tuning (not LoRA)
deepspeed: examples/deepspeed/ds_z3_partial_offload_config.json

### dataset
dataset: tbench_traces_sharegptv1
template: qwen  # Qwen3 template (same as Qwen2.5)
mask_history: true
cutoff_len: 196608
max_samples: null  # Use full dataset
overwrite_cache: false
preprocessing_num_workers: 8
dataloader_num_workers: 4

### output
output_dir: output/qwen3_full_sft_192k_alst
logging_steps: 5
save_steps: 50
plot_loss: true
overwrite_output_dir: true
save_only_model: true  # Save only model for DeepSpeed compatibility
report_to: wandb  # Set to 'none' to disable WandB

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 4  # Increased due to longer sequences
learning_rate: 1.5e-5  # Slightly lower LR for stability with long context
num_train_epochs: 3.0  # Reduced epochs due to computational cost
lr_scheduler_type: cosine
warmup_ratio: 0.1  # Longer warmup for stability
weight_decay: 0.03
max_grad_norm: 1.0e-3
bf16: true
pure_bf16: true # bf16: true leaves parameters in fp32
gradient_checkpointing: true
ddp_timeout: 100000
