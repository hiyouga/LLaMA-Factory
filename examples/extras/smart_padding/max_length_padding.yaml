# Configuration for sequence length padding options
# These are required for sequence parallelism/ALST training to ensure
# all sequences have exactly the same length across all ranks

# Option 1: Fixed length padding - all sequences padded to cutoff_len
force_sequence_length_padding: true

# Option 2: Dynamic max length padding - all sequences padded to longest sample in batch
# force_max_length_padding: true

# Set cutoff_len to desired sequence length
# This should be divisible by sequence_parallel_size for optimal performance
cutoff_len: 4096

# For sequence parallel training, ensure cutoff_len is divisible by sequence_parallel_size
# Example: cutoff_len=4096, sequence_parallel_size=4 â†’ each rank gets 1024 tokens

# Note: force_sequence_length_padding and force_max_length_padding are mutually exclusive
