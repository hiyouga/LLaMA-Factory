![# LLaMA Factory](assets/logo.png)

[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)
[![GitHub Code License](https://img.shields.io/github/license/hiyouga/LLaMA-Factory)](LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)
[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)
[![Citation](https://img.shields.io/badge/citation-72-green)](#llama-factory-ã‚’ä½¿ç”¨ã—ãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ)
[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/hiyouga/LLaMA-Factory/pulls)
[![Discord](https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&style=flat)](https://discord.gg/rKfvV9r9FK)
[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)
[![Open in DSW](https://gallery.pai-ml.com/assets/open-in-dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)
[![Spaces](https://img.shields.io/badge/ğŸ¤—-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)
[![Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)

[![GitHub Tread](https://trendshift.io/api/badge/repositories/4535)](https://trendshift.io/repositories/4535)

ğŸ‘‹ [WeChat](assets/wechat.jpg) ã¾ãŸã¯ [NPUãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—](assets/wechat_npu.jpg) ã«å‚åŠ ã—ã¦ãã ã•ã„ã€‚

\[ [English](README.md) | [ä¸­æ–‡](README_zh.md) | æ—¥æœ¬èª \]

**å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ç°¡å˜ã§ã™...**

https://github.com/hiyouga/LLaMA-Factory/assets/16256802/9840a653-7e9c-41c8-ae89-7ace5698baf6

é¸æŠè‚¢ï¼š

- **Colab**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing
- **PAI-DSW**: https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory
- **ãƒ­ãƒ¼ã‚«ãƒ«ãƒã‚·ãƒ³**: [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•) ã‚’å‚ç…§ã—ã¦ãã ã•ã„

## ç›®æ¬¡

- [ç‰¹å¾´](#ç‰¹å¾´)
- [ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯](#ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯)
- [å¤‰æ›´å±¥æ­´](#å¤‰æ›´å±¥æ­´)
- [ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«](#ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«)
- [ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ](#ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ)
- [æä¾›ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](#æä¾›ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ)
- [è¦ä»¶](#è¦ä»¶)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
- [LLaMA Factoryã‚’ä½¿ç”¨ã—ãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ](#llama-factory-ã‚’ä½¿ç”¨ã—ãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ)
- [ãƒ©ã‚¤ã‚»ãƒ³ã‚¹](#ãƒ©ã‚¤ã‚»ãƒ³ã‚¹)
- [å¼•ç”¨](#å¼•ç”¨)
- [è¬è¾](#è¬è¾)

## ç‰¹å¾´

- **ã•ã¾ã–ã¾ãªãƒ¢ãƒ‡ãƒ«**: LLaMAã€LLaVAã€Mistralã€Mixtral-MoEã€Qwenã€Yiã€Gemmaã€Baichuanã€ChatGLMã€Phiãªã©ã€‚
- **çµ±åˆã•ã‚ŒãŸæ–¹æ³•**: ï¼ˆé€£ç¶šï¼‰äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€ï¼ˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ï¼‰æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€å ±é…¬ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€PPOã€DPOã€KTOã€ORPOãªã©ã€‚
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒªã‚½ãƒ¼ã‚¹**: 16ãƒ“ãƒƒãƒˆã®ãƒ•ãƒ«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ãƒ•ãƒªãƒ¼ã‚ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€LoRAã€ãŠã‚ˆã³AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQã«ã‚ˆã‚‹2/3/4/5/6/8ãƒ“ãƒƒãƒˆã®QLoRAã€‚
- **é«˜åº¦ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: GaLoreã€BAdamã€DoRAã€LongLoRAã€LLaMA Proã€Mixture-of-Depthsã€LoRA+ã€LoftQã€PiSSAã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚
- **å®Ÿç”¨çš„ãªãƒˆãƒªãƒƒã‚¯**: FlashAttention-2ã€Unslothã€RoPEã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€NEFTuneã€rsLoRAã€‚
- **å®Ÿé¨“ãƒ¢ãƒ‹ã‚¿ãƒ¼**: LlamaBoardã€TensorBoardã€Wandbã€MLflowãªã©ã€‚
- **é«˜é€Ÿãªæ¨è«–**: OpenAIã‚¹ã‚¿ã‚¤ãƒ«ã®APIã€Gradio UIã€CLIã¨vLLMãƒ¯ãƒ¼ã‚«ãƒ¼ã€‚

## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

ChatGLMã®[P-Tuning](https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning)ã¨æ¯”è¼ƒã—ã¦ã€LLaMA Factoryã®LoRAãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€åºƒå‘Šãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¿ã‚¹ã‚¯ã§**3.7å€ã®é«˜é€ŸåŒ–**ã‚’æä¾›ã—ã€ã‚ˆã‚Šé«˜ã„Rougeã‚¹ã‚³ã‚¢ã‚’é”æˆã—ã¾ã™ã€‚4ãƒ“ãƒƒãƒˆé‡å­åŒ–æŠ€è¡“ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€LLaMA Factoryã®QLoRAã¯GPUãƒ¡ãƒ¢ãƒªã®åŠ¹ç‡ã‚’ã•ã‚‰ã«å‘ä¸Šã•ã›ã¾ã™ã€‚

![benchmark](assets/benchmark.svg)

<details><summary>å®šç¾©</summary>

- **ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦**: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«1ç§’ã‚ãŸã‚Šã«å‡¦ç†ã•ã‚Œã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ã€‚ï¼ˆbs=4ã€cutoff_len=1024ï¼‰
- **Rougeã‚¹ã‚³ã‚¢**: [åºƒå‘Šãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ](https://aclanthology.org/D19-1321.pdf)ã‚¿ã‚¹ã‚¯ã®é–‹ç™ºã‚»ãƒƒãƒˆã§ã®Rouge-2ã‚¹ã‚³ã‚¢ã€‚ï¼ˆbs=4ã€cutoff_len=1024ï¼‰
- **GPUãƒ¡ãƒ¢ãƒª**: 4ãƒ“ãƒƒãƒˆé‡å­åŒ–ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã®ãƒ”ãƒ¼ã‚¯GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€‚ï¼ˆbs=1ã€cutoff_len=1024ï¼‰
- ChatGLMã®P-Tuningã«ã¯`pre_seq_len=128`ã‚’ã€LLaMA Factoryã®LoRAãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯`lora_rank=32`ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚

</details>

## å¤‰æ›´å±¥æ­´

[24/06/16] **[PiSSA](https://arxiv.org/abs/2404.02948)**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[examples](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[24/06/07] **[Qwen2](https://qwenlm.github.io/blog/qwen2/)**ãŠã‚ˆã³**[GLM-4](https://github.com/THUDM/GLM-4)**ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚

[24/05/26] **[SimPO](https://arxiv.org/abs/2405.14734)**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[examples](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

<details><summary>å®Œå…¨ãªå¤‰æ›´å±¥æ­´</summary>

[24/05/20] **PaliGemma**ã‚·ãƒªãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚PaliGemmaãƒ¢ãƒ‡ãƒ«ã¯äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ãƒãƒ£ãƒƒãƒˆè£œå®Œã®ãŸã‚ã«`gemma`ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

[24/05/18] **[KTO](https://arxiv.org/abs/2402.01306)**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[examples](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[24/05/14] æ˜‡é¨°NPUãƒ‡ãƒã‚¤ã‚¹ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚è©³ç´°ã¯[ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[24/04/26] **LLaVA-1.5**ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[examples](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[24/04/22] ç„¡æ–™ã®T4 GPUã§Llama-3ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®**[Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)**ã‚’æä¾›ã—ã¾ã—ãŸã€‚LLaMA Factoryã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸ2ã¤ã®Llama-3æ´¾ç”Ÿãƒ¢ãƒ‡ãƒ«ãŒHugging Faceã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚è©³ç´°ã¯[Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat)ãŠã‚ˆã³[Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[24/04/21] **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)**ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚[AstraMindAIã®å®Ÿè£…](https://github.com/astramind-ai/Mixture-of-depths)ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[examples](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[24/04/16] **[BAdam](https://arxiv.org/abs/2404.02827)**ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[examples](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[24/04/16] **[unsloth](https://github.com/unslothai/unsloth)**ã®é•·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆLlama-2-7B-56kã‚’24GBã§å®Ÿè¡Œï¼‰ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚FlashAttention-2ã¨æ¯”è¼ƒã—ã¦**117%**ã®é€Ÿåº¦ã¨**50%**ã®ãƒ¡ãƒ¢ãƒªã‚’é”æˆã—ã¾ã™ã€‚è©³ç´°ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯[ã“ã®ãƒšãƒ¼ã‚¸](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison)ã§ç¢ºèªã§ãã¾ã™ã€‚

[24/03/31] **[ORPO](https://arxiv.org/abs/2403.07691)**ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[examples](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[24/03/21] ç§ãŸã¡ã®è«–æ–‡"[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)"ãŒarXivã§å…¬é–‹ã•ã‚Œã¾ã—ãŸï¼

[24/03/20] **FSDP+QLoRA**ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚70Bãƒ¢ãƒ‡ãƒ«ã‚’2x24GB GPUã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã¾ã™ã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[examples](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[24/03/13] **[LoRA+](https://arxiv.org/abs/2402.12354)**ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[examples](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[24/03/07] å‹¾é…ä½ãƒ©ãƒ³ã‚¯æŠ•å½±ï¼ˆ**[GaLore](https://arxiv.org/abs/2403.03507)**ï¼‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[examples](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[24/03/07] **[vLLM](https://github.com/vllm-project/vllm)**ã‚’çµ±åˆã—ã€ã‚ˆã‚Šé«˜é€Ÿã§åŒæ™‚å®Ÿè¡Œå¯èƒ½ãªæ¨è«–ã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚`infer_backend: vllm`ã‚’è©¦ã—ã¦**270%**ã®æ¨è«–é€Ÿåº¦ã‚’ä½“é¨“ã—ã¦ãã ã•ã„ã€‚

[24/02/28] é‡ã¿åˆ†è§£LoRAï¼ˆ**[DoRA](https://arxiv.org/abs/2402.09353)**ï¼‰ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚`use_dora: true`ã‚’è©¦ã—ã¦DoRAãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚

[24/02/15] **LLaMA Pro**ãŒææ¡ˆã™ã‚‹**ãƒ–ãƒ­ãƒƒã‚¯æ‹¡å¼µ**ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[examples](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[24/02/05] Qwen1.5ï¼ˆQwen2ãƒ™ãƒ¼ã‚¿ç‰ˆï¼‰ã‚·ãƒªãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ãŒLLaMA-Factoryã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¾ã—ãŸã€‚è©³ç´°ã¯[ã“ã®ãƒ–ãƒ­ã‚°æŠ•ç¨¿](https://qwenlm.github.io/blog/qwen1.5/)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[24/01/18] ã»ã¨ã‚“ã©ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦**ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚`dataset: glaive_toolcall_en`ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€ãƒ„ãƒ¼ãƒ«ä½¿ç”¨èƒ½åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«ä»˜ä¸ã§ãã¾ã™ã€‚

[23/12/23] **[unsloth](https://github.com/unslothai/unsloth)**ã®å®Ÿè£…ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€LLaMAã€Mistralã€Yiãƒ¢ãƒ‡ãƒ«ã®LoRAãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–ã—ã¾ã—ãŸã€‚`use_unsloth: true`å¼•æ•°ã‚’è©¦ã—ã¦unslothãƒ‘ãƒƒãƒã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã¯**170%**ã®é€Ÿåº¦ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚è©³ç´°ã¯[ã“ã®ãƒšãƒ¼ã‚¸](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison)ã§ç¢ºèªã§ãã¾ã™ã€‚

[23/12/12] æœ€æ–°ã®MoEãƒ¢ãƒ‡ãƒ«**[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)**ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶ã«ã¤ã„ã¦ã¯[ã“ã¡ã‚‰](#ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[23/12/01] **[ModelScope Hub](https://modelscope.cn/models)**ã‹ã‚‰äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](#modelscope-hubã‹ã‚‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[23/10/21] **[NEFTune](https://arxiv.org/abs/2310.05914)**ãƒˆãƒªãƒƒã‚¯ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚`neftune_noise_alpha: 5`å¼•æ•°ã‚’è©¦ã—ã¦NEFTuneã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚

[23/09/27] LLaMAãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦[LongLoRA](https://github.com/dvlab-research/LongLoRA)ãŒææ¡ˆã™ã‚‹**$S^2$-Attn**ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚`shift_attn: true`å¼•æ•°ã‚’è©¦ã—ã¦ã‚·ãƒ•ãƒˆã‚·ãƒ§ãƒ¼ãƒˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚

[23/09/23] ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã«MMLUã€C-Evalã€CMMLUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’çµ±åˆã—ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[examples](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[23/09/10] **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚RTX4090ã€A100ã€H100 GPUã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ã€`flash_attn: fa2`å¼•æ•°ã‚’è©¦ã—ã¦FlashAttention-2ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚

[23/08/12] LLaMAãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã®**RoPEã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã«`rope_scaling: linear`å¼•æ•°ã‚’ã€æ¨è«–æ™‚ã«`rope_scaling: dynamic`å¼•æ•°ã‚’è©¦ã—ã¦ä½ç½®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’å¤–æŒ¿ã—ã¦ãã ã•ã„ã€‚

[23/08/11] **[DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°](https://arxiv.org/abs/2305.18290)**ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[examples](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[23/07/31] **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°**ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚`streaming: true`ãŠã‚ˆã³`max_steps: 10000`å¼•æ•°ã‚’è©¦ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

[23/07/29] Hugging Faceã§2ã¤ã®13BæŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚è©³ç´°ã¯ã“ã‚Œã‚‰ã®Hugging Faceãƒªãƒã‚¸ãƒˆãƒªï¼ˆ[LLaMA-2](https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat) / [Baichuan](https://huggingface.co/hiyouga/Baichuan-13B-sft)ï¼‰ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[23/07/18] ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€è©•ä¾¡ã€æ¨è«–ã®ãŸã‚ã®**ã‚ªãƒ¼ãƒ«ã‚¤ãƒ³ãƒ¯ãƒ³Web UI**ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚`train_web.py`ã‚’è©¦ã—ã¦Webãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ãã ã•ã„ã€‚é–‹ç™ºã«ãŠã„ã¦[@KanadeSiina](https://github.com/KanadeSiina)ãŠã‚ˆã³[@codemayq](https://github.com/codemayq)ã®åŠªåŠ›ã«æ„Ÿè¬ã—ã¾ã™ã€‚

[23/07/09] **[FastEdit](https://github.com/hiyouga/FastEdit)** âš¡ğŸ©¹ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®äº‹å®ŸçŸ¥è­˜ã‚’åŠ¹ç‡çš„ã«ç·¨é›†ã™ã‚‹ãŸã‚ã®ä½¿ã„ã‚„ã™ã„ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã™ã€‚èˆˆå‘³ãŒã‚ã‚‹å ´åˆã¯[FastEdit](https://github.com/hiyouga/FastEdit)ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ã—ã¦ãã ã•ã„ã€‚

[23/06/29] æŒ‡ç¤ºã«å¾“ã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®**å†ç¾å¯èƒ½ãªä¾‹**ã‚’æä¾›ã—ã¾ã—ãŸã€‚è©³ç´°ã¯[Baichuan-7B-sft](https://huggingface.co/hiyouga/Baichuan-7B-sft)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

[23/06/22] [ãƒ‡ãƒ¢API](src/api_demo.py)ã‚’[OpenAIã®](https://platform.openai.com/docs/api-reference/chat)å½¢å¼ã«åˆã‚ã›ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€**ä»»æ„ã®ChatGPTãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³**ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æŒ¿å…¥ã§ãã¾ã™ã€‚

[23/06/03] é‡å­åŒ–ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ï¼ˆåˆ¥å**[QLoRA](https://github.com/artidoro/qlora)**ï¼‰ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯[examples](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

</details>

## ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«

| ãƒ¢ãƒ‡ãƒ«                                                        | ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º                       | ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ  |
| ------------------------------------------------------------ | -------------------------------- | --------- |
| [Baichuan 2](https://huggingface.co/baichuan-inc)            | 7B/13B                           | baichuan2 |
| [BLOOM/BLOOMZ](https://huggingface.co/bigscience)            | 560M/1.1B/1.7B/3B/7.1B/176B      | -         |
| [ChatGLM3](https://huggingface.co/THUDM)                     | 6B                               | chatglm3  |
| [Command R](https://huggingface.co/CohereForAI)              | 35B/104B                         | cohere    |
| [DeepSeek (Code/MoE)](https://huggingface.co/deepseek-ai)    | 7B/16B/67B/236B                  | deepseek  |
| [Falcon](https://huggingface.co/tiiuae)                      | 7B/11B/40B/180B                  | falcon    |
| [Gemma/Gemma 2/CodeGemma](https://huggingface.co/google)     | 2B/7B/9B/27B                     | gemma     |
| [GLM-4](https://huggingface.co/THUDM)                        | 9B                               | glm4      |
| [InternLM2](https://huggingface.co/internlm)                 | 7B/20B                           | intern2   |
| [Llama](https://github.com/facebookresearch/llama)           | 7B/13B/33B/65B                   | -         |
| [Llama 2](https://huggingface.co/meta-llama)                 | 7B/13B/70B                       | llama2    |
| [Llama 3/Llama 3.1](https://huggingface.co/meta-llama)       | 8B/70B                           | llama3    |
| [LLaVA-1.5](https://huggingface.co/llava-hf)                 | 7B/13B                           | vicuna    |
| [Mistral/Mixtral](https://huggingface.co/mistralai)          | 7B/8x7B/8x22B                    | mistral   |
| [OLMo](https://huggingface.co/allenai)                       | 1B/7B                            | -         |
| [PaliGemma](https://huggingface.co/google)                   | 3B                               | gemma     |
| [Phi-1.5/Phi-2](https://huggingface.co/microsoft)            | 1.3B/2.7B                        | -         |
| [Phi-3](https://huggingface.co/microsoft)                    | 4B/7B/14B                        | phi       |
| [Qwen/Qwen1.5/Qwen2 (Code/MoE)](https://huggingface.co/Qwen) | 0.5B/1.5B/4B/7B/14B/32B/72B/110B | qwen      |
| [StarCoder 2](https://huggingface.co/bigcode)                | 3B/7B/15B                        | -         |
| [XVERSE](https://huggingface.co/xverse)                      | 7B/13B/65B                       | xverse    |
| [Yi/Yi-1.5](https://huggingface.co/01-ai)                    | 6B/9B/34B                        | yi        |
| [Yi-VL](https://huggingface.co/01-ai)                        | 6B/34B                           | yi_vl     |
| [Yuan 2](https://huggingface.co/IEITYuan)                    | 2B/51B/102B                      | yuan      |

> [!NOTE]
> "ãƒ™ãƒ¼ã‚¹"ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€`template`å¼•æ•°ã¯`default`ã€`alpaca`ã€`vicuna`ãªã©ã‹ã‚‰é¸æŠã§ãã¾ã™ã€‚ãŸã ã—ã€"æŒ‡ç¤º/ãƒãƒ£ãƒƒãƒˆ"ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯**å¯¾å¿œã™ã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
>
> ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã§**åŒã˜**ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚

ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã®å®Œå…¨ãªãƒªã‚¹ãƒˆã«ã¤ã„ã¦ã¯ã€[constants.py](src/llamafactory/extras/constants.py)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’[template.py](src/llamafactory/data/template.py)ã«è¿½åŠ ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

## ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

| ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ               |     ãƒ•ãƒ«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°    |    ãƒ•ãƒªãƒ¼ã‚ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°   |       LoRA         |       QLoRA        |
| ---------------------- | ------------------ | ------------------ | ------------------ | ------------------ |
| äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |
| æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |
| å ±é…¬ãƒ¢ãƒ‡ãƒªãƒ³ã‚°        | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |
| PPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |
| DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |
| KTOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |
| ORPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°          | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |
| SimPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°         | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |

## æä¾›ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

<details><summary>äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</summary>

- [Wiki Demo (en)](data/wiki_demo.txt)
- [RefinedWeb (en)](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)
- [RedPajama V2 (en)](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)
- [Wikipedia (en)](https://huggingface.co/datasets/olm/olm-wikipedia-20221220)
- [Wikipedia (zh)](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)
- [Pile (en)](https://huggingface.co/datasets/EleutherAI/pile)
- [SkyPile (zh)](https://huggingface.co/datasets/Skywork/SkyPile-150B)
- [FineWeb (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb)
- [FineWeb-Edu (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)
- [The Stack (en)](https://huggingface.co/datasets/bigcode/the-stack)
- [StarCoder (en)](https://huggingface.co/datasets/bigcode/starcoderdata)

</details>

<details><summary>æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</summary>

- [Identity (en&zh)](data/identity.json)
- [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)
- [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)
- [Alpaca GPT4 (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)
- [Glaive Function Calling V2 (en&zh)](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2)
- [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)
- [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)
- [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)
- [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)
- [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)
- [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)
- [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)
- [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)
- [UltraChat (en)](https://github.com/thunlp/UltraChat)
- [OpenPlatypus (en)](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)
- [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)
- [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)
- [OpenOrca (en)](https://huggingface.co/datasets/Open-Orca/OpenOrca)
- [SlimOrca (en)](https://huggingface.co/datasets/Open-Orca/SlimOrca)
- [MathInstruct (en)](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)
- [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)
- [Wiki QA (en)](https://huggingface.co/datasets/wiki_qa)
- [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)
- [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)
- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)
- [deepctrl (en&zh)](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)
- [Advertise Generating (zh)](https://huggingface.co/datasets/HasturOfficial/adgen)
- [ShareGPT Hyperfiltered (en)](https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k)
- [ShareGPT4 (en&zh)](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)
- [UltraChat 200k (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)
- [AgentInstruct (en)](https://huggingface.co/datasets/THUDM/AgentInstruct)
- [LMSYS Chat 1M (en)](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)
- [Evol Instruct V2 (en)](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)
- [Cosmopedia (en)](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)
- [STEM (zh)](https://huggingface.co/datasets/hfl/stem_zh_instruction)
- [Ruozhiba (zh)](https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo)
- [Neo-sft (zh)](https://huggingface.co/datasets/m-a-p/neo_sft_phase2)
- [WebInstructSub (en)](https://huggingface.co/datasets/TIGER-Lab/WebInstructSub)
- [Magpie-Pro-300K-Filtered (en)](https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered)
- [LLaVA mixed (en&zh)](https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k)
- [Open Assistant (de)](https://huggingface.co/datasets/mayflowergmbh/oasst_de)
- [Dolly 15k (de)](https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de)
- [Alpaca GPT4 (de)](https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de)
- [OpenSchnabeltier (de)](https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de)
- [Evol Instruct (de)](https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de)
- [Dolphin (de)](https://huggingface.co/datasets/mayflowergmbh/dolphin_de)
- [Booksum (de)](https://huggingface.co/datasets/mayflowergmbh/booksum_de)
- [Airoboros (de)](https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de)
- [Ultrachat (de)](https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de)

</details>

<details><summary>å„ªå…ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</summary>

- [DPO mixed (en&zh)](https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k)
- [UltraFeedback (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)
- [Orca DPO Pairs (en)](https://huggingface.co/datasets/Intel/orca_dpo_pairs)
- [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)
- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)
- [Orca DPO (de)](https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de)
- [KTO mixed (en)](https://huggingface.co/datasets/argilla/kto-mix-15k)

</details>

ä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä½¿ç”¨å‰ã«ç¢ºèªãŒå¿…è¦ã§ã™ã€‚ãã®ãŸã‚ã€Hugging Faceã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

```bash
pip install --upgrade huggingface_hub
huggingface-cli login
```

## è¦ä»¶

| å¿…é ˆ    | æœ€å° | æ¨å¥¨ |
| ------------ | ------- | --------- |
| python       | 3.8     | 3.11      |
| torch        | 1.13.1  | 2.3.0     |
| transformers | 4.41.2  | 4.41.2    |
| datasets     | 2.16.0  | 2.19.2    |
| accelerate   | 0.30.1  | 0.30.1    |
| peft         | 0.11.1  | 0.11.1    |
| trl          | 0.8.6   | 0.9.4     |

| ã‚ªãƒ—ã‚·ãƒ§ãƒ³     | æœ€å° | æ¨å¥¨ |
| ------------ | ------- | --------- |
| CUDA         | 11.6    | 12.2      |
| deepspeed    | 0.10.0  | 0.14.0    |
| bitsandbytes | 0.39.0  | 0.43.1    |
| vllm         | 0.4.3   | 0.4.3     |
| flash-attn   | 2.3.0   | 2.5.9     |

### ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶

\* *æ¨å®šå€¤*

| æ–¹æ³•            | ãƒ“ãƒƒãƒˆ |   7B  |  13B  |  30B  |   70B  |  110B  |  8x7B |  8x22B |
| ----------------- | ---- | ----- | ----- | ----- | ------ | ------ | ----- | ------ |
| ãƒ•ãƒ«              | AMP  | 120GB | 240GB | 600GB | 1200GB | 2000GB | 900GB | 2400GB |
| ãƒ•ãƒ«              |  16  |  60GB | 120GB | 300GB |  600GB |  900GB | 400GB | 1200GB |
| ãƒ•ãƒªãƒ¼ã‚º            |  16  |  20GB |  40GB |  80GB |  200GB |  360GB | 160GB |  400GB |
| LoRA/GaLore/BAdam |  16  |  16GB |  32GB |  64GB |  160GB |  240GB | 120GB |  320GB |
| QLoRA             |   8  |  10GB |  20GB |  40GB |   80GB |  140GB |  60GB |  160GB |
| QLoRA             |   4  |   6GB |  12GB |  24GB |   48GB |   72GB |  30GB |   96GB |
| QLoRA             |   2  |   4GB |   8GB |  16GB |   24GB |   48GB |  18GB |   48GB |

## ä½¿ç”¨æ–¹æ³•

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

> [!IMPORTANT]
> ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¯å¿…é ˆã§ã™ã€‚

```bash
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]"
```

åˆ©ç”¨å¯èƒ½ãªè¿½åŠ ä¾å­˜é–¢ä¿‚: torchã€torch-npuã€metricsã€deepspeedã€bitsandbytesã€hqqã€eetqã€gptqã€awqã€aqlmã€vllmã€galoreã€badamã€qwenã€modelscopeã€quality

> [!TIP]
> ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ç«¶åˆã‚’è§£æ±ºã™ã‚‹ã«ã¯ã€`pip install --no-deps -e .`ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

<details><summary>Windowsãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘</summary>

Windowsãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§é‡å­åŒ–LoRAï¼ˆQLoRAï¼‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹å ´åˆã€äº‹å‰ã«ãƒ“ãƒ«ãƒ‰ã•ã‚ŒãŸ`bitsandbytes`ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚CUDA 11.1ã‹ã‚‰12.2ã¾ã§ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«å¿œã˜ãŸé©åˆ‡ãª[ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ã‚¸ãƒ§ãƒ³](https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels)ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

```bash
pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
```

Windowsãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§FlashAttention-2ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€äº‹å‰ã«ãƒ“ãƒ«ãƒ‰ã•ã‚ŒãŸ`flash-attn`ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚CUDA 12.1ã‹ã‚‰12.2ã¾ã§ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚å¿…è¦ã«å¿œã˜ã¦[flash-attention](https://github.com/bdashore3/flash-attention/releases)ã‹ã‚‰å¯¾å¿œã™ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

</details>

<details><summary>æ˜‡é¨°NPUãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘</summary>

æ˜‡é¨°NPUãƒ‡ãƒã‚¤ã‚¹ã§LLaMA Factoryã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã«ã¯ã€è¿½åŠ ã®ä¾å­˜é–¢ä¿‚ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼š`pip install -e ".[torch-npu,metrics]"`ã€‚ã•ã‚‰ã«ã€**[Ascend CANN Toolkit and Kernels](https://www.hiascend.com/developer/download/community/result?module=cann)**ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚[ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html)ã«å¾“ã†ã‹ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼š

```bash
# URLã‚’CANNãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ãƒ‡ãƒã‚¤ã‚¹ã«å¿œã˜ã¦ç½®ãæ›ãˆã¦ãã ã•ã„
# CANN Toolkitã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-"$(uname -i)".run
bash Ascend-cann-toolkit_8.0.RC1

# CANN Kernelsã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run
bash Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run --install

# envå¤‰æ•°ã‚’ã‚»ãƒƒãƒˆ
source /usr/local/Ascend/ascend-toolkit/set_env.sh
```

| å¿…é ˆ  | æœ€å° | æ¨å¥¨   |
| ------------ | ------- | ----------- |
| CANN         | 8.0.RC1 | 8.0.RC1     |
| torch        | 2.1.0   | 2.1.0       |
| torch-npu    | 2.1.0   | 2.1.0.post3 |
| deepspeed    | 0.13.2  | 0.13.2      |

ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã®æŒ‡å®šã«ã¯ `CUDA_VISIBLE_DEVICES` ã§ã¯ãªã `ASCEND_RT_VISIBLE_DEVICES` ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚

NPUãƒ‡ãƒã‚¤ã‚¹ã§ãƒ¢ãƒ‡ãƒ«ã‚’æ¨æ¸¬ã§ããªã„å ´åˆã¯ã€ã‚³ãƒ³ãƒ•ã‚£ã‚®ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§`do_sample: false`ã‚’è¨­å®šã—ã¦ã¿ã¦ãã ã•ã„ã€‚

ãƒ“ãƒ«ãƒ‰æ¸ˆã¿Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: [32GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html) | [64GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html)

</details>

### ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã¤ã„ã¦ã¯ã€[data/README.md](data/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚Hugging Face / ModelScopeã®ãƒãƒ–ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

> [!NOTE]
> ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã« `data/dataset_info.json` ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚

### ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

Llama3-8B-Instructãƒ¢ãƒ‡ãƒ«ã®LoRAã® **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**ã€**æ¨è«–**ã€**ãƒãƒ¼ã‚¸** ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€ãã‚Œãã‚Œä»¥ä¸‹ã®3ã¤ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```bash
llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
```

é«˜åº¦ãªä½¿ã„æ–¹ï¼ˆåˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å«ã‚€ï¼‰ã«ã¤ã„ã¦ã¯[examples/README.md](examples/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

> [!TIP]
> ãƒ˜ãƒ«ãƒ—æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯ `llamafactory-cli help` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

### LLaMAãƒœãƒ¼ãƒ‰GUIã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆPowered by [Gradio](https://github.com/gradio-app/gradio)

```bash
llamafactory-cli webui
```

### Dockerã®ãƒ“ãƒ«ãƒ‰

CUDAãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘:

```bash
cd docker/docker-cuda/
docker-compose up -d
docker-compose exec llamafactory bash
```

ã‚¢ã‚»ãƒ³ãƒ‰NPUãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘:

```bash
cd docker/docker-npu/
docker-compose up -d
docker-compose exec llamafactory bash
```

<details><summary>Docker Composeã‚’ä½¿ã‚ãªã„ãƒ“ãƒ«ãƒ‰</summary>

CUDAãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘:

```bash
docker build -f ./docker/docker-cuda/Dockerfile \
    --build-arg INSTALL_BNB=false \
    --build-arg INSTALL_VLLM=false \
    --build-arg INSTALL_DEEPSPEED=false \
    --build-arg INSTALL_FLASHATTN=false \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    -t llamafactory:latest .

docker run -dit --gpus=all \
    -v ./hf_cache:/root/.cache/huggingface \
    -v ./ms_cache:/root/.cache/modelscope \
    -v ./data:/app/data \
    -v ./output:/app/output \
    -p 7860:7860 \
    -p 8000:8000 \
    --shm-size 16G \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
```

ã‚¢ã‚»ãƒ³ãƒ‰NPUãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘:

```bash
# ã‚ãªãŸã®ç’°å¢ƒã«åˆã‚ã›ã¦dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’é¸æŠ
docker build -f ./docker/docker-npu/Dockerfile \
    --build-arg INSTALL_DEEPSPEED=false \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    -t llamafactory:latest .

# ãƒªã‚½ãƒ¼ã‚¹ã«å¿œã˜ã¦`device`ã‚’å¤‰æ›´
docker run -dit \
    -v ./hf_cache:/root/.cache/huggingface \
    -v ./ms_cache:/root/.cache/modelscope \
    -v ./data:/app/data \
    -v ./output:/app/output \
    -v /usr/local/dcmi:/usr/local/dcmi \
    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
    -v /etc/ascend_install.info:/etc/ascend_install.info \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/davinci0 \
    --device /dev/davinci_manager \
    --device /dev/devmm_svm \
    --device /dev/hisi_hdc \
    --shm-size 16G \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
```

</details>

<details><summary>ãƒœãƒªãƒ¥ãƒ¼ãƒ ã®è©³ç´°</summary>

- hf_cache: ãƒ›ã‚¹ãƒˆãƒã‚·ãƒ³ã®Hugging Faceã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ã™ã‚‹ã€‚åˆ¥ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æ—¢ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€å†å‰²ã‚Šå½“ã¦å¯èƒ½ã€‚
- data: LLaMA Board GUIã§é¸æŠã§ãã‚‹ã‚ˆã†ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ›ã‚¹ãƒˆãƒã‚·ãƒ³ã®ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç½®ãã€‚
- output: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå…ˆã‚’ã“ã®å ´æ‰€ã«è¨­å®šã™ã‚‹ã“ã¨ã§ã€ãƒãƒ¼ã‚¸ã•ã‚ŒãŸçµæœã«ãƒ›ã‚¹ãƒˆãƒã‚·ãƒ³ã‹ã‚‰ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚

</details>

### OpenAIã‚¹ã‚¿ã‚¤ãƒ«ã®APIã¨vLLMã‚’ä½¿ã£ãŸãƒ‡ãƒ—ãƒ­ã‚¤

```bash
API_PORT=8000 llamafactory-cli api examples/inference/llama3_vllm.yaml
```

> [!TIP]
> APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ https://platform.openai.com/docs/api-reference/chat/createã€‚

### ModelScope Hubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

Hugging Faceã‹ã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å•é¡ŒãŒã‚ã‚‹å ´åˆã¯ã€ModelScopeã‚’ã”åˆ©ç”¨ãã ã•ã„ã€‚

```bash
export USE_MODELSCOPE_HUB=1 # Windowsã®å ´åˆã€`set USE_MODELSCOPE_HUB=1`
```

ModelScope Hubã®ãƒ¢ãƒ‡ãƒ«IDã‚’ `model_name_or_path` ã«æŒ‡å®šã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã€‚ãƒ¢ãƒ‡ãƒ«IDã®å®Œå…¨ãªãƒªã‚¹ãƒˆã¯[ModelScope Hub](https://modelscope.cn/models)ã«ã‚ã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€`LLM-Research/Meta-Llama-3-8B-Instruct`ã§ã™ã€‚

### W&Bãƒ­ã‚¬ãƒ¼ã®ä½¿ç”¨

å®Ÿé¨“çµæœã®ãƒ­ã‚®ãƒ³ã‚°ã«[Weights & Biases](https://wandb.ai)ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€yamlãƒ•ã‚¡ã‚¤ãƒ«ã«ä»¥ä¸‹ã®å¼•æ•°ã‚’è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```yaml
report_to: wandb
run_name: test_run # ã‚ªãƒ—ã‚·ãƒ§ãƒ³
```

W&Bã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ãŸã‚ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã‚’èµ·å‹•ã™ã‚‹éš›ã«ã€`WANDB_API_KEY`ã‚’[ã‚ãªãŸã®ã‚­ãƒ¼](https://wandb.ai/authorize)ã«è¨­å®šã—ã¾ã™ã€‚

## LLaMAãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã‚’ä½¿ç”¨ã—ãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

å–ã‚Šè¾¼ã‚€ã¹ããƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒã‚ã‚Šã¾ã—ãŸã‚‰ã€ãƒ¡ãƒ¼ãƒ«ã§ã”é€£çµ¡ã„ãŸã ãã‹ã€ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

<details><summary>ã‚¯ãƒªãƒƒã‚¯ã§è¡¨ç¤º</summary>

1. Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. [[arxiv]](https://arxiv.org/abs/2308.02223)
1. Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. [[arxiv]](https://arxiv.org/abs/2308.10092)
1. Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. [[arxiv]](https://arxiv.org/abs/2308.10526)
1. Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. [[arxiv]](https://arxiv.org/abs/2311.07816)
1. Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. [[arxiv]](https://arxiv.org/abs/2312.15710)
1. Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. [[arxiv]](https://arxiv.org/abs/2401.04319)
1. Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2401.07286)
1. Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2402.05904)
1. Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. [[arxiv]](https://arxiv.org/abs/2402.07625)
1. Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11176)
1. Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. [[arxiv]](https://arxiv.org/abs/2402.11187)
1. Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. [[arxiv]](https://arxiv.org/abs/2402.11746)
1. Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11801)
1. Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2402.11809)
1. Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11819)
1. Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. [[arxiv]](https://arxiv.org/abs/2402.12204)
1. Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.14714)
1. Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. [[arxiv]](https://arxiv.org/abs/2402.15043)
1. Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. [[arxiv]](https://arxiv.org/abs/2403.02333)
1. Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. [[arxiv]](https://arxiv.org/abs/2403.03419)
1. Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2403.08228)
1. Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2403.09073)
1. Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. [[arxiv]](https://arxiv.org/abs/2403.14541)
1. Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2403.15246)
1. Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. [[arxiv]](https://arxiv.org/abs/2403.16008)
1. Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. [[arxiv]](https://arxiv.org/abs/2403.16443)
1. Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2404.00604)
1. Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.02827)
1. Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2404.04167)
1. Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. [[arxiv]](https://arxiv.org/abs/2404.04316)
1. Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.07084)
1. Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.09836)
1. Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.11581)
1. Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. [[arxiv]](https://arxiv.org/abs/2404.14215)
1. Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2404.16621)
1. Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2404.17140)
1. Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. [[arxiv]](https://arxiv.org/abs/2404.18585)
1. Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. [[arxiv]](https://arxiv.org/abs/2405.04760)
1. Dammu et al. "They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. [[arxiv]](https://arxiv.org/abs/2405.05378)
1. Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. [[arxiv]](https://arxiv.org/abs/2405.09055)
1. Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. [[arxiv]](https://arxiv.org/abs/2405.12739)
1. Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2405.13816)
1. Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2405.20215)
1. Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. [[paper]](https://aclanthology.org/2024.lt4hala-1.30)
1. Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2406.00380)
1. Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. [[arxiv]](https://arxiv.org/abs/2406.02106)
1. Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. [[arxiv]](https://arxiv.org/abs/2406.03136)
1. Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2406.04496)
1. Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. [[arxiv]](https://arxiv.org/abs/2406.05688)
1. Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. [[arxiv]](https://arxiv.org/abs/2406.05955)
1. Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. [[arxiv]](https://arxiv.org/abs/2406.06973)
1. Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. [[arxiv]](https://arxiv.org/abs/2406.07115)
1. Zhu et al. Are Large Language Models Good Statisticians?. 2024. [[arxiv]](https://arxiv.org/abs/2406.07815)
1. Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2406.10099)
1. Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. [[arxiv]](https://arxiv.org/abs/2406.10173)
1. He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. [[arxiv]](https://arxiv.org/abs/2406.12074)
1. Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. [[arxiv]](https://arxiv.org/abs/2406.14408)
1. Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. [[arxiv]](https://arxiv.org/abs/2406.14546)
1. Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. [[arxiv]](https://arxiv.org/abs/2406.15695)
1. Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. [[arxiv]](https://arxiv.org/abs/2406.17233)
1. Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. [[arxiv]](https://arxiv.org/abs/2406.18069)
1. Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburghâ€™s Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. [[paper]](https://aclanthology.org/2024.americasnlp-1.25)
1. **[StarWhisper](https://github.com/Yu-Yang-Li/StarWhisper)**: ChatGLM2-6Bã¨Qwen-14Bã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸå¤©æ–‡å­¦ç”¨ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€‚
1. **[DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)**: Baichuan-13Bã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸä¸­å›½æ³•é ˜åŸŸã«ç‰¹åŒ–ã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã€æ³•çŸ¥è­˜ã®æ¤œç´¢ã¨æ¨è«–ãŒå¯èƒ½ã§ã‚ã‚‹ã€‚
1. **[Sunsimiao](https://github.com/X-D-Lab/Sunsimiao)**: Baichuan-7Bã¨ChatGLM-6Bã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸã€ä¸­å›½èªåŒ»ç™‚åˆ†é‡ã«ç‰¹åŒ–ã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€‚
1. **[CareGPT](https://github.com/WangRongsheng/CareGPT)**: LLaMA2-7Bã¨Baichuan-13Bã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸä¸­å›½èªåŒ»ç™‚åˆ†é‡ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚ºã€‚
1. **[MachineMindset](https://github.com/PKU-YuanGroup/Machine-Mindset/)**: MBTIãƒ‘ãƒ¼ã‚½ãƒŠãƒªãƒ†ã‚£å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚·ãƒªãƒ¼ã‚ºã§ã€ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ–¹æ³•ã«åŸºã¥ã„ã¦ã€16ã®ç•°ãªã‚‹ãƒ‘ãƒ¼ã‚½ãƒŠãƒªãƒ†ã‚£ã‚¿ã‚¤ãƒ—ã‚’LLMã«ä¸ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
1. **[Luminia-13B-v3](https://huggingface.co/Nekochu/Luminia-13B-v3)**: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã«ç‰¹åŒ–ã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§å®‰å®šã—ãŸæ™®åŠã‚’ç›®æŒ‡ã™ã€‚[[ğŸ¤—ãƒ‡ãƒ¢]](https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt)
1. **[Chinese-LLaVA-Med](https://github.com/BUAADreamer/Chinese-LLaVA-Med)**: LLaVA-1.5-7Bã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸã€ä¸­å›½èªåŒ»ç™‚é ˜åŸŸã«ç‰¹åŒ–ã—ãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€‚
1. **[AutoRE](https://github.com/THUDM/AutoRE)**: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ãæ–‡æ›¸ãƒ¬ãƒ™ãƒ«é–¢ä¿‚æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ã€‚
1. **[NVIDIA RTX AI Toolkit](https://github.com/NVIDIA/RTX-AI-Toolkit)**: NVIDIA RTXç”¨ã®Windows PCä¸Šã§LLMã‚’å¾®èª¿æ•´ã™ã‚‹ãŸã‚ã®SDKã€‚
1. **[LazyLLM](https://github.com/LazyAGI/LazyLLM)**: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆLLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç°¡å˜ã‹ã¤å®¹æ˜“ã«æ§‹ç¯‰ã§ãã€LLaMAãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

</details>

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯[Apache-2.0 License](LICENSE)ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

å¯¾å¿œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã‚¦ã‚§ã‚¤ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¾“ã£ã¦ãã ã•ã„: [Baichuan 2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf) / [BLOOM](https://huggingface.co/spaces/bigscience/license) / [ChatGLM3](https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE) / [Command R](https://cohere.com/c4ai-cc-by-nc-license) / [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL) / [Falcon](https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt) / [Gemma](https://ai.google.dev/gemma/terms) / [GLM-4](https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE) / [InternLM2](https://github.com/InternLM/InternLM#license) / [Llama](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) / [Llama 2 (LLaVA-1.5)](https://ai.meta.com/llama/license/) / [Llama 3](https://llama.meta.com/llama3/license/) / [Mistral](LICENSE) / [OLMo](LICENSE) / [Phi-1.5/Phi-2](https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx) / [Phi-3](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE) / [Qwen](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) / [StarCoder 2](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) / [XVERSE](https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf) / [Yi](https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE) / [Yi-1.5](LICENSE) / [Yuan 2](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/LICENSE-Yuan)

## å¼•ç”¨

ã‚‚ã—ã“ã®ç ”ç©¶ãŒãŠå½¹ã«ç«‹ã¤ã‚ˆã†ã§ã—ãŸã‚‰ã€ä»¥ä¸‹ã®ã‚ˆã†ã«å¼•ç”¨ã—ã¦ãã ã•ã„:

```bibtex
@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}
```

## è¬è¾

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€[PEFT](https://github.com/huggingface/peft)ã€[TRL](https://github.com/huggingface/trl)ã€[QLoRA](https://github.com/artidoro/qlora)ã€[FastChat](https://github.com/lm-sys/FastChat)ã®æ©æµã‚’å—ã‘ã¦ã„ã¾ã™ã€‚å½¼ã‚‰ã®ç´ æ™´ã‚‰ã—ã„ä»•äº‹ã«æ„Ÿè¬ã—ã¾ã™ã€‚

## Star History

![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&type=Date)
